{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import face_recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PICS_PATH = 'materials/pictures/'\n",
    "\n",
    "IMG1 = PICS_PATH+'jb1.png'\n",
    "IMG2 = PICS_PATH+'jb2.png'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Detection\n",
    "To "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def detect_and_crop_face(image_path):\n",
    "    # Load Haar Cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(\n",
    "        cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'\n",
    "    )\n",
    "\n",
    "    # Read the image\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Convert to grayscale\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces in the image\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray_image,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5\n",
    "    )\n",
    "\n",
    "    # Check if any faces were found\n",
    "    if len(faces) == 0:\n",
    "        print(\"No faces found.\")\n",
    "        return None\n",
    "\n",
    "    faces_pics = []\n",
    "    # Crop and save each detected face\n",
    "    for (x, y, w, h) in faces:\n",
    "        face = image[y:y+h, x:x+w]  # Crop the face from the image\n",
    "        faces_pics.append(face)\n",
    "\n",
    "    return faces_pics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage example\n",
    "faces_pics1 = detect_and_crop_face(IMG1)\n",
    "\n",
    "len(faces_pics1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage example\n",
    "faces_pics2 = detect_and_crop_face(IMG2)\n",
    "\n",
    "len(faces_pics2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Encoding\n",
    "The next step is to extract features from these faces. This can be done using facial embeddings or encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings = []\n",
    "\n",
    "for pic1 in faces_pics1:\n",
    "    image1_rgb = cv2.cvtColor(pic1, cv2.COLOR_BGR2RGB)\n",
    "    encoding1 = face_recognition.face_encodings(image1_rgb)\n",
    "    for pic2 in faces_pics2:\n",
    "        image2_rgb = cv2.cvtColor(pic2, cv2.COLOR_BGR2RGB)\n",
    "        encoding2 = face_recognition.face_encodings(image2_rgb)\n",
    "\n",
    "        if len(encoding1) > 0 and len(encoding2) > 0:\n",
    "            encodings.append([encoding1[0], encoding2[0]])\n",
    "\n",
    "len(encodings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.01751508,  0.07707559, -0.02128401, -0.02624768, -0.0507114 ,\n",
       "       -0.01067236,  0.01568273, -0.03122935,  0.06684347, -0.08669632,\n",
       "        0.22173631, -0.04362556, -0.25420994, -0.0246059 ,  0.05292112,\n",
       "        0.12277183, -0.16477899, -0.16083917, -0.22634934, -0.1034404 ,\n",
       "        0.10632364, -0.01822525, -0.0114993 ,  0.04524447, -0.23437706,\n",
       "       -0.27861577, -0.07145165, -0.10128508,  0.0903349 , -0.14441672,\n",
       "        0.01707909, -0.03193469, -0.1455247 , -0.07796918,  0.02380305,\n",
       "        0.00708509, -0.04617143, -0.09996295,  0.16534936,  0.09301843,\n",
       "       -0.20689608,  0.08410276, -0.00193592,  0.33988947,  0.29497528,\n",
       "        0.06139373,  0.07164442, -0.06280392,  0.01383303, -0.26104403,\n",
       "        0.04817868,  0.12485193,  0.13748586,  0.09956717,  0.09879835,\n",
       "       -0.11403167,  0.00589237,  0.11198126, -0.19326156,  0.0511108 ,\n",
       "        0.04213471, -0.04731822,  0.01416795, -0.10521804,  0.13353063,\n",
       "        0.0422986 , -0.11264879, -0.09896166,  0.20074968, -0.20758063,\n",
       "       -0.12592031,  0.01583915, -0.11964306, -0.14973915, -0.34239364,\n",
       "       -0.0292809 ,  0.35948199,  0.19081354, -0.21070838, -0.0012107 ,\n",
       "       -0.01141465,  0.02589972,  0.05805591,  0.10110425, -0.06501321,\n",
       "       -0.02811785, -0.03504586,  0.0680878 ,  0.16346893, -0.03718309,\n",
       "       -0.05177827,  0.18926311,  0.00597238, -0.11841787,  0.10866334,\n",
       "        0.06537898, -0.13197878, -0.03493074, -0.0789925 , -0.04938418,\n",
       "        0.02206787, -0.11099217,  0.03775874,  0.14731392, -0.15000838,\n",
       "        0.19282787, -0.06019697, -0.02593839, -0.02157521,  0.05915751,\n",
       "       -0.05850223, -0.03124434,  0.15491268, -0.2282103 ,  0.09780427,\n",
       "        0.33428288, -0.01924969,  0.03004255,  0.04250683,  0.06268109,\n",
       "       -0.037012  ,  0.08475289, -0.24187417, -0.09339792, -0.05680326,\n",
       "       -0.00505663,  0.03440695,  0.02256563])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encodings[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Face Comparison\n",
    "To compare two faces, calculate the distance between their embeddings. A common metric is the Euclidean distance. If the distance is below a certain threshold (e.g., 0.6), the faces are likely to be of the same person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.40255146180208334\n",
      "The two images are likely of the same person.\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "max_dist = 1.0\n",
    "idx = -1\n",
    "\n",
    "# Calculate Euclidean distance between two encodings\n",
    "for i, val in enumerate(encodings):\n",
    "    encoding1, encoding2 = val\n",
    "    dist = distance.euclidean(encoding1, encoding2)\n",
    "\n",
    "    if dist < max_dist:\n",
    "        max_dist = dist\n",
    "        idx = i\n",
    "\n",
    "\n",
    "# Define a threshold for determining if they are the same person\n",
    "threshold = 0.6\n",
    "\n",
    "print(max_dist)\n",
    "\n",
    "if dist < threshold:\n",
    "    print(\"The two images are likely of the same person.\")\n",
    "else:\n",
    "    print(\"The two images are likely of different people.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 7495.5450525480865\n"
     ]
    }
   ],
   "source": [
    "def mse(imageA, imageB, is_resize: bool=False):\n",
    "    # Ensure the images have the same size\n",
    "    if imageA.shape != imageB.shape:\n",
    "        if not is_resize:\n",
    "            assert imageA.shape == imageB.shape, \"Images must be the same size.\"\n",
    "        imageB = cv2.resize(imageB, (imageA.shape[1], imageA.shape[0]))\n",
    "    \n",
    "    # Calculate the MSE between the images\n",
    "    err = np.sum((imageA.astype(\"float\") - imageB.astype(\"float\")) ** 2)\n",
    "    err /= float(imageA.shape[0] * imageA.shape[1])\n",
    "    \n",
    "    return err\n",
    "\n",
    "# Load images\n",
    "imageA = cv2.imread(IMG1, cv2.IMREAD_GRAYSCALE)\n",
    "imageB = cv2.imread(IMG2, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Compute MSE\n",
    "error = mse(imageA, imageB, is_resize=True)\n",
    "print(f\"Mean Squared Error: {error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SSIM Score: 0.0589125209112179\n"
     ]
    }
   ],
   "source": [
    "from skimage.metrics import structural_similarity as ssim\n",
    "import cv2\n",
    "\n",
    "def compare_ssim(imageA, imageB, is_resize: bool=False):\n",
    "    # Ensure the images have the same size\n",
    "    if imageA.shape != imageB.shape:\n",
    "        if not is_resize:\n",
    "            assert imageA.shape == imageB.shape, \"Images must be the same size.\"\n",
    "        imageB = cv2.resize(imageB, (imageA.shape[1], imageA.shape[0]))\n",
    "    \n",
    "    # Compute SSIM between two images\n",
    "    score, diff = ssim(imageA, imageB, full=True)\n",
    "    return score\n",
    "\n",
    "# Load images\n",
    "imageA = cv2.imread(IMG1, cv2.IMREAD_GRAYSCALE)\n",
    "imageB = cv2.imread(IMG2, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "# Compute SSIM\n",
    "ssim_score = compare_ssim(imageA, imageB, is_resize=True)\n",
    "print(f\"SSIM Score: {ssim_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Histogram Comparison Score (Correlation): 0.03436912205227033\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def compare_histograms(imageA, imageB, method='correlation'):\n",
    "    # Convert images to HSV color space for better comparison\n",
    "    imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2HSV)\n",
    "    imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Calculate histograms\n",
    "    histA = cv2.calcHist([imageA], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
    "    histB = cv2.calcHist([imageB], [0, 1], None, [50, 60], [0, 180, 0, 256])\n",
    "\n",
    "    # Normalize histograms\n",
    "    cv2.normalize(histA, histA, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "    cv2.normalize(histB, histB, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "\n",
    "    # Use correlation or other methods\n",
    "    methods = {\n",
    "        'correlation': cv2.HISTCMP_CORREL,\n",
    "        'chi-square': cv2.HISTCMP_CHISQR,\n",
    "        'bhattacharyya': cv2.HISTCMP_BHATTACHARYYA\n",
    "    }\n",
    "\n",
    "    comparison = cv2.compareHist(histA, histB, methods[method])\n",
    "    return comparison\n",
    "\n",
    "# Load images\n",
    "imageA = cv2.imread(IMG1)\n",
    "imageB = cv2.imread(IMG2)\n",
    "\n",
    "# Compare histograms using correlation\n",
    "hist_score = compare_histograms(imageA, imageB, method='correlation')\n",
    "print(f\"Histogram Comparison Score (Correlation): {hist_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Matches: 127\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "def orb_feature_compare(imageA, imageB):\n",
    "    # Convert to grayscale\n",
    "    imageA = cv2.cvtColor(imageA, cv2.COLOR_BGR2GRAY)\n",
    "    imageB = cv2.cvtColor(imageB, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Initialize ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Find the keypoints and descriptors with ORB\n",
    "    kpA, desA = orb.detectAndCompute(imageA, None)\n",
    "    kpB, desB = orb.detectAndCompute(imageB, None)\n",
    "\n",
    "    # Match descriptors\n",
    "    bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "    matches = bf.match(desA, desB)\n",
    "    \n",
    "    # Sort matches by distance\n",
    "    matches = sorted(matches, key=lambda x: x.distance)\n",
    "    \n",
    "    return len(matches), matches\n",
    "\n",
    "# Load images\n",
    "imageA = cv2.imread(IMG1)\n",
    "imageB = cv2.imread(IMG2)\n",
    "\n",
    "# Compare features using ORB\n",
    "matches_count, matches = orb_feature_compare(imageA, imageB)\n",
    "print(f\"Number of Matches: {matches_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)\n",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n",
      "\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models, transforms\n",
      "\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Load pre-trained ResNet model\n",
    "model = models.resnet50(pretrained=True)\n",
    "model = nn.Sequential(*list(model.children())[:-1])  # Remove the last classification layer\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "    img = Image.open(image_path)\n",
    "    img = preprocess(img).unsqueeze(0)  # Add batch dimension\n",
    "    return img\n",
    "\n",
    "def cosine_similarity(featureA, featureB):\n",
    "    cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "    return cos(featureA, featureB)\n",
    "\n",
    "# Load and preprocess images\n",
    "imageA = preprocess_image(IMG1)\n",
    "imageB = preprocess_image(IMG2)\n",
    "\n",
    "# Extract features\n",
    "with torch.no_grad():\n",
    "    featuresA = model(imageA)\n",
    "    featuresB = model(imageB)\n",
    "\n",
    "# Compare using cosine similarity\n",
    "similarity = cosine_similarity(featuresA, featuresB)\n",
    "print(f\"Cosine Similarity: {similarity.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Una niña\n"
     ]
    }
   ],
   "source": [
    "from transliterate import translit\n",
    "\n",
    "# Original Russian text\n",
    "russian_text = \"Una niña\"\n",
    "\n",
    "# Transliterate to English\n",
    "english_text = translit(russian_text, 'ru', reversed=True)\n",
    "print(english_text)  # Output: Privet, kak dela?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'aaa__aa.fffgf'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text = 'aaa  aa.fffgf.gf'\n",
    "\n",
    "a = text[:text.rfind('.')]\n",
    "a = a.replace(' ', '_')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "# text = \"Bonjour tout le monde\"\n",
    "text =  \"Una niña\"\n",
    "text =  \"Скриншот\"\n",
    "text =  \"abFdAff\"\n",
    "text = \"привет\"\n",
    "\n",
    "# Transliterate text\n",
    "translated = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "print(translated)  # Output: Hello\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# Load images\n",
    "\n",
    "image1 = cv2.imread(IMG1)\n",
    "image2 = cv2.imread(IMG2)\n",
    "\n",
    "hist_img1 = cv2.calcHist([image1], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "hist_img1[255, 255, 255] = 0 #ignore all white pixels\n",
    "cv2.normalize(hist_img1, hist_img1, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "hist_img2 = cv2.calcHist([image2], [0, 1, 2], None, [256, 256, 256], [0, 256, 0, 256, 0, 256])\n",
    "hist_img2[255, 255, 255] = 0  #ignore all white pixels\n",
    "cv2.normalize(hist_img2, hist_img2, alpha=0, beta=1, norm_type=cv2.NORM_MINMAX)\n",
    "# Find the metric value\n",
    "metric_val = cv2.compareHist(hist_img1, hist_img2, cv2.HISTCMP_CORREL)\n",
    "print(f\"Similarity Score: \", round(metric_val, 2))\n",
    "# Similarity Score: 0.94"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(246, 205, 3) (246, 205, 3)\n",
      "SSIM Score:  0.06\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from skimage import metrics\n",
    "# Load images\n",
    "image1 = cv2.imread(IMG1)\n",
    "image2 = cv2.imread(IMG2)\n",
    "image2 = cv2.resize(image2, (image1.shape[1], image1.shape[0]), interpolation = cv2.INTER_AREA)\n",
    "print(image1.shape, image2.shape)\n",
    "# Convert images to grayscale\n",
    "image1_gray = cv2.cvtColor(image1, cv2.COLOR_BGR2GRAY)\n",
    "image2_gray = cv2.cvtColor(image2, cv2.COLOR_BGR2GRAY)\n",
    "# Calculate SSIM\n",
    "ssim_score = metrics.structural_similarity(image1_gray, image2_gray, full=True)\n",
    "print(f\"SSIM Score: \", round(ssim_score[0], 2))\n",
    "# SSIM Score: 0.38"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "class SingletonClass:\n",
    "    _instance = None\n",
    "\n",
    "    def __new__(cls):\n",
    "        if cls._instance is None:\n",
    "            cls._instance = super(SingletonClass, cls).__new__(cls)\n",
    "        return cls._instance\n",
    "\n",
    "# Usage\n",
    "singleton1 = SingletonClass()\n",
    "singleton2 = SingletonClass()\n",
    "print(singleton1 is singleton2)  # Output: True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.2, 'B'), (2.5, 'A'), (3.8, 'C')]\n"
     ]
    }
   ],
   "source": [
    "def add_and_sort(distances, new_distance, new_id):\n",
    "    # Append the new tuple (distance, id) to the list\n",
    "    distances.append((new_distance, new_id))\n",
    "    \n",
    "    # Sort the list of tuples based on the first element (distance)\n",
    "    distances.sort(key=lambda x: x[0])  # Sort by distance (first element of tuple)\n",
    "    \n",
    "    return distances\n",
    "\n",
    "# Example usage\n",
    "sorted_distances = []\n",
    "\n",
    "# Simulating adding new elements\n",
    "sorted_distances = add_and_sort(sorted_distances, 2.5, 'A')\n",
    "sorted_distances = add_and_sort(sorted_distances, 1.2, 'B')\n",
    "sorted_distances = add_and_sort(sorted_distances, 3.8, 'C')\n",
    "\n",
    "print(sorted_distances)  # Output: [(1.2, 'B'), (2.5, 'A'), (3.8, 'C')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1.2, 'B'), (2.5, 'A'), (3.8, 'C')]\n"
     ]
    }
   ],
   "source": [
    "import bisect\n",
    "\n",
    "\n",
    "def add_sorted(distances, new_distance, new_id):\n",
    "    # Create a tuple for the new element\n",
    "    new_tuple = (new_distance, new_id)\n",
    "\n",
    "    # Use bisect.insort to insert while maintaining order\n",
    "    bisect.insort(distances, new_tuple)\n",
    "\n",
    "\n",
    "# Example usage\n",
    "sorted_distances = []\n",
    "\n",
    "add_sorted(sorted_distances, 2.5, 'A')\n",
    "add_sorted(sorted_distances, 1.2, 'B')\n",
    "add_sorted(sorted_distances, 3.8, 'C')\n",
    "\n",
    "print(sorted_distances)  # Output: [(1.2, 'B'), (2.5, 'A'), (3.8, 'C')]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".g_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
